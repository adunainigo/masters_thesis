{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2024263a-c568-4691-9a57-a367c32834d6",
   "metadata": {},
   "source": [
    "# First approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f66253-e542-4174-9816-9fc513c2185c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a9bfefd3-b7fb-4827-9d82-57b2550c9644",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aduna\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aduna\\Documents\\Master_KU_Leuven\\Master_Thesis\\Code\\masters_thesis\\tests\\mlp.py:219: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  pred_labels_train = torch.tensor(pred_labels_train, dtype=torch.float).squeeze()\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './lr_1e-3_h_200-200_best_model.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[206], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToTensorV2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UNET\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLP\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_checkpoint,save_predictions_as_imgs\n",
      "File \u001b[1;32m~\\Documents\\Master_KU_Leuven\\Master_Thesis\\Code\\masters_thesis\\tests\\mlp.py:230\u001b[0m\n\u001b[0;32m    227\u001b[0m model5 \u001b[38;5;241m=\u001b[39m MLP(INPUT_SIZE, HIDDEN_SIZE1, OUTPUT_SIZE)\n\u001b[0;32m    228\u001b[0m model6 \u001b[38;5;241m=\u001b[39m MLP(INPUT_SIZE, HIDDEN_SIZE2, OUTPUT_SIZE)\n\u001b[1;32m--> 230\u001b[0m model1\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(MODEL_PATH_1))\n\u001b[0;32m    231\u001b[0m model2\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(MODEL_PATH_2))\n\u001b[0;32m    232\u001b[0m model3\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(MODEL_PATH_3))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './lr_1e-3_h_200-200_best_model.tar'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "import os \n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from model import UNET\n",
    "from mlp import MLP\n",
    "from PIL import Image\n",
    "from utils import load_checkpoint,save_predictions_as_imgs\n",
    "from torchvision.utils import save_image\n",
    "from postprocessing import preprocess_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede2b204-552a-4744-978c-d185e38b16ff",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d04df30f-fe8c-432a-abb0-c7d02c334fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#MODEL WEIGHTS\n",
    "UNET_WEIGHTS_PATH = \"./my_checkpoint.pth.tar\"\n",
    "\n",
    "MLP_1 =        \"./pretrained/lr_1e-3_h_200-200_best_model.tar\"\n",
    "MLP_2 =        \"./pretrained/lr_1e-3_h_250-250_best_model.tar\"\n",
    "MLP_3 =        \"./pretrained/lr_1e-4_h_200-200_best_model.tar\"\n",
    "MLP_4 =        \"./pretrained/lr_1e-4_h_250-250_best_model.tar\"\n",
    "MLP_5 =        \"./pretrained/lr_1e-5_h_200-200_best_model.tar\"\n",
    "MLP_6 =        \"./pretrained/lr_1e-5_h_250-250_best_model.tar\"\n",
    "\n",
    "#IMAGE_DIRECTORIES\n",
    "PHOTO = \"./data_1a/photo\"\n",
    "PHOTO_MASK = \"./data_1a/photo_mask\"\n",
    "PHOTO_MASK_LABEL = \"./data_1a/photo_mask_label\"\n",
    "PRED_SCALE_FACTORS = \"./scale_factors_pred.txt\"\n",
    "\n",
    "#IMAGE PROPERTIES (U_NET)\n",
    "IMAGE_HEIGHT = 270 #135,270,540,1080\n",
    "IMAGE_WIDTH  = 480  #240,480,960,1920\n",
    "\n",
    "#MODEL_PROPERTIES (MLP)\n",
    "INPUT_SIZE= 8\n",
    "OUTPUT_SIZE = 3\n",
    "HIDDEN_SIZE1 = [200,200]\n",
    "HIDDEN_SIZE2 = [250,250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca727718-0e9f-4c48-8e70-381dac3f137d",
   "metadata": {},
   "source": [
    "## Image2Mask (U-Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb850c32-2e1e-4879-ae46-38b678a3a252",
   "metadata": {},
   "source": [
    "### U_NET DEFINITION AND WEIGHT DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "743984be-9a22-468d-98ed-94575dde9ab1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "model = UNET(in_channels=3, out_channels=1).to(DEVICE);\n",
    "load_checkpoint(torch.load(UNET_WEIGHTS_PATH, map_location=DEVICE), model);\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a57c9b-5f61-4dbd-a04f-e38a9d4cdb6f",
   "metadata": {},
   "source": [
    "### Image Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c129fc87-acee-46d7-bece-449feec3377d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST TRANSFORMATIONS\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab3264d-dc59-4d2f-a5ee-846bee7f8f4d",
   "metadata": {},
   "source": [
    "### IMAGE2MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0c8dfdc0-8862-4777-af1d-75a034c485a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()  # Start time for performance measurement\n",
    "\n",
    "# Get the single image file from the PHOTO directory\n",
    "image_file = os.listdir(PHOTO)[1]\n",
    "# Extract the index from the filename\n",
    "match = re.search(r'image_(\\d+).jpg', image_file)\n",
    "if match:\n",
    "    index = int(match.group(1))\n",
    "    image_path = os.path.join(PHOTO, image_file)\n",
    "    image = np.array(Image.open(image_path))\n",
    "    \n",
    "    # Transform and predict\n",
    "    transformed = test_transforms(image=image)\n",
    "    image = transformed[\"image\"].unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(image)\n",
    "    predictions = torch.sigmoid(predictions)\n",
    "    predictions = (predictions > 0.5).float()\n",
    "    \n",
    "    # Save the predicted mask\n",
    "    filepath = os.path.join(PHOTO_MASK, f\"pred_mask_{index}.jpg\")\n",
    "    save_image(predictions, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e907ca-5480-403f-a5d9-32ae3b38cf25",
   "metadata": {},
   "source": [
    "### MASK2LABEL (Camera_Frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "10134751-5e51-42d2-afce-ff7467f71c59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_labels(PRED_MASK_DIR, SAVING_FOLDER):\n",
    "    \"\"\"\n",
    "    Loads the single data mask file, predicts the labels, and saves it in a specified folder as label_i.txt.\n",
    "    \"\"\"\n",
    "    # Assume there's only one mask file in the directory\n",
    "    mask_file = os.listdir(PRED_MASK_DIR)[1]\n",
    "    match = re.match(r'pred_mask_(\\d+)\\.jpg', mask_file)\n",
    "    if match:\n",
    "        i = int(match.group(1))\n",
    "        mask_path = os.path.join(PRED_MASK_DIR, mask_file)\n",
    "        pieces_features = preprocess_image(mask_path)  # Assuming preprocess_image is defined elsewhere\n",
    "\n",
    "        # Construct the saving path for the label file\n",
    "        save_path = os.path.join(SAVING_FOLDER, f'label_{i}.txt')\n",
    "\n",
    "        # Save the predicted features to a file\n",
    "        with open(save_path, 'w') as file:\n",
    "            json.dump(pieces_features, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "efc59adc-a086-4c40-8dcc-36ee6968769f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_labels(PHOTO_MASK,PHOTO_MASK_LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ef5df-9046-4e5d-8000-e12df9765b93",
   "metadata": {},
   "source": [
    "### Label2Normalise (Camera_Frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "00b043e1-bc0e-4722-a5bc-9c301b4f1202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalise_json(input_dir, scale_factors_filename):\n",
    "    # Buscar el primer archivo .txt en el directorio\n",
    "    json_file = next((f for f in os.listdir(input_dir) if f.endswith('.txt')), None)\n",
    "    if not json_file:\n",
    "        raise FileNotFoundError(\"No .txt file found in the directory.\")\n",
    "    json_path = os.path.join(input_dir, json_file)\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    with open(scale_factors_filename, 'r') as file:\n",
    "        scale_factors = json.load(file)\n",
    "    \n",
    "    for entry in data:\n",
    "        for key, value in entry.items():\n",
    "            if key == 'centroid':\n",
    "                x_key, y_key = 'centroid_x', 'centroid_y'\n",
    "                entry[key][0] = normalize_value(entry[key][0], scale_factors.get(x_key, {}))\n",
    "                entry[key][1] = normalize_value(entry[key][1], scale_factors.get(y_key, {}))\n",
    "            else:\n",
    "                if key in scale_factors and isinstance(value, (int, float)):\n",
    "                    entry[key] = normalize_value(value, scale_factors[key])\n",
    "    with open(json_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "def normalize_value(value, factor):\n",
    "    if not factor:  \n",
    "        return value\n",
    "    norm_value = (value - factor['mean']) / factor['std'] if factor['std'] else value - factor['mean']\n",
    "    return 2 * (norm_value - factor['min_after_std']) / (factor['max_after_std'] - factor['min_after_std']) - 1 if factor['max_after_std'] - factor['min_after_std'] else norm_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "039f0ef4-ce8c-4bb8-9ead-0d744dcb06d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalise_json(PHOTO_MASK_LABEL, PRED_SCALE_FACTORS);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bfe181-c6bf-4217-b370-9c6439e81a08",
   "metadata": {},
   "source": [
    "### CameraFrame2RobotFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "deb2a44e-5360-48e6-a1fc-dcb89705af0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[205], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model1 \u001b[38;5;241m=\u001b[39m MLP(INPUT_SIZE, HIDDEN_SIZE1, OUTPUT_SIZE)\n\u001b[0;32m      2\u001b[0m model2 \u001b[38;5;241m=\u001b[39m MLP(INPUT_SIZE, HIDDEN_SIZE2, OUTPUT_SIZE)\n\u001b[0;32m      3\u001b[0m model3 \u001b[38;5;241m=\u001b[39m MLP(INPUT_SIZE, HIDDEN_SIZE1, OUTPUT_SIZE)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MLP' is not defined"
     ]
    }
   ],
   "source": [
    "model1 = MLP(INPUT_SIZE, HIDDEN_SIZE1, OUTPUT_SIZE)\n",
    "model2 = MLP(INPUT_SIZE, HIDDEN_SIZE2, OUTPUT_SIZE)\n",
    "model3 = MLP(INPUT_SIZE, HIDDEN_SIZE1, OUTPUT_SIZE)\n",
    "model4 = MLP(INPUT_SIZE, HIDDEN_SIZE2, OUTPUT_SIZE)\n",
    "model5 = MLP(INPUT_SIZE, HIDDEN_SIZE1, OUTPUT_SIZE)\n",
    "model6 = MLP(INPUT_SIZE, HIDDEN_SIZE2, OUTPUT_SIZE)\n",
    "\n",
    "model1.load_state_dict(torch.load(MODEL_1))\n",
    "model2.load_state_dict(torch.load(MODEL_2))\n",
    "model3.load_state_dict(torch.load(MODEL_3))\n",
    "model4.load_state_dict(torch.load(MODEL_4))\n",
    "model5.load_state_dict(torch.load(MODEL_5))\n",
    "model6.load_state_dict(torch.load(MODEL_6))\n",
    "\n",
    "model1.eval() \n",
    "model2.eval() \n",
    "model3.eval() \n",
    "model4.eval() \n",
    "model5.eval() \n",
    "model6.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5c2f5-0219-4287-b357-27e79ce7abf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
