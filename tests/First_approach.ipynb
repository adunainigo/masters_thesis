{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2024263a-c568-4691-9a57-a367c32834d6",
   "metadata": {},
   "source": [
    "# First approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f66253-e542-4174-9816-9fc513c2185c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a9bfefd3-b7fb-4827-9d82-57b2550c9644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "import os \n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from model import UNET\n",
    "from mlp import MLP,read_predicted_labels\n",
    "from PIL import Image\n",
    "from utils import load_checkpoint,save_predictions_as_imgs\n",
    "from torchvision.utils import save_image\n",
    "from mask2label import preprocess_image,normalise_json2,reconstruct_output2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede2b204-552a-4744-978c-d185e38b16ff",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d04df30f-fe8c-432a-abb0-c7d02c334fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#MODEL WEIGHTS\n",
    "UNET_WEIGHTS_PATH = \"./my_checkpoint.pth.tar\"\n",
    "\n",
    "MLP_1 =        \"./pretrained/lr_1e-3_h_200-200_best_model.tar\"\n",
    "MLP_2 =        \"./pretrained/lr_1e-3_h_250-250_best_model.tar\"\n",
    "MLP_3 =        \"./pretrained/lr_1e-4_h_200-200_best_model.tar\"\n",
    "MLP_4 =        \"./pretrained/lr_1e-4_h_250-250_best_model.tar\"\n",
    "MLP_5 =        \"./pretrained/lr_1e-5_h_200-200_best_model.tar\"\n",
    "MLP_6 =        \"./pretrained/lr_1e-5_h_250-250_best_model.tar\"\n",
    "\n",
    "#IMAGE_DIRECTORIES\n",
    "PHOTO = \"./data_1a/photo\"\n",
    "PHOTO_MASK = \"./data_1a/photo_mask\"\n",
    "PHOTO_MASK_LABEL = \"./data_1a/photo_mask_label\"\n",
    "PRED_SCALE_FACTORS = \"./scale_factors_pred.txt\"\n",
    "GT_SCALE_FACTORS = \"./scale_factors_gt.txt\"\n",
    "#IMAGE PROPERTIES (U_NET)\n",
    "IMAGE_HEIGHT = 270 #135,270,540,1080\n",
    "IMAGE_WIDTH  = 480  #240,480,960,1920\n",
    "\n",
    "#MODEL_PROPERTIES (MLP)\n",
    "INPUT_SIZE= 8\n",
    "OUTPUT_SIZE = 3\n",
    "HIDDEN_SIZE1 = [200,200]\n",
    "HIDDEN_SIZE2 = [250,250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca727718-0e9f-4c48-8e70-381dac3f137d",
   "metadata": {},
   "source": [
    "## Image2Mask (U-Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb850c32-2e1e-4879-ae46-38b678a3a252",
   "metadata": {},
   "source": [
    "### U_NET DEFINITION AND WEIGHT DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "743984be-9a22-468d-98ed-94575dde9ab1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "model = UNET(in_channels=3, out_channels=1).to(DEVICE);\n",
    "load_checkpoint(torch.load(UNET_WEIGHTS_PATH, map_location=DEVICE), model);\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a57c9b-5f61-4dbd-a04f-e38a9d4cdb6f",
   "metadata": {},
   "source": [
    "### Image Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c129fc87-acee-46d7-bece-449feec3377d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST TRANSFORMATIONS\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab3264d-dc59-4d2f-a5ee-846bee7f8f4d",
   "metadata": {},
   "source": [
    "### IMAGE2MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c8dfdc0-8862-4777-af1d-75a034c485a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()  # Start time for performance measurement\n",
    "\n",
    "# Get the single image file from the PHOTO directory\n",
    "image_file = os.listdir(PHOTO)[1]\n",
    "# Extract the index from the filename\n",
    "match = re.search(r'image_(\\d+).jpg', image_file)\n",
    "if match:\n",
    "    index = int(match.group(1))\n",
    "    image_path = os.path.join(PHOTO, image_file)\n",
    "    image = np.array(Image.open(image_path))\n",
    "    \n",
    "    # Transform and predict\n",
    "    transformed = test_transforms(image=image)\n",
    "    image = transformed[\"image\"].unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(image)\n",
    "    predictions = torch.sigmoid(predictions)\n",
    "    predictions = (predictions > 0.5).float()\n",
    "    \n",
    "    # Save the predicted mask\n",
    "    filepath = os.path.join(PHOTO_MASK, f\"pred_mask_{index}.jpg\")\n",
    "    save_image(predictions, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e907ca-5480-403f-a5d9-32ae3b38cf25",
   "metadata": {},
   "source": [
    "### MASK2LABEL (Camera_Frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "10134751-5e51-42d2-afce-ff7467f71c59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_labels(PRED_MASK_DIR, SAVING_FOLDER):\n",
    "    \"\"\"\n",
    "    Loads the single data mask file, predicts the labels, and saves it in a specified folder as label_i.txt.\n",
    "    \"\"\"\n",
    "    # Assume there's only one mask file in the directory\n",
    "    mask_file = os.listdir(PRED_MASK_DIR)[1]\n",
    "    match = re.match(r'pred_mask_(\\d+)\\.jpg', mask_file)\n",
    "    if match:\n",
    "        i = int(match.group(1))\n",
    "        mask_path = os.path.join(PRED_MASK_DIR, mask_file)\n",
    "        pieces_features = preprocess_image(mask_path)  # Assuming preprocess_image is defined elsewhere\n",
    "\n",
    "        # Construct the saving path for the label file\n",
    "        save_path = os.path.join(SAVING_FOLDER, f'label_{i}.txt')\n",
    "\n",
    "        # Save the predicted features to a file\n",
    "        with open(save_path, 'w') as file:\n",
    "            json.dump(pieces_features, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "efc59adc-a086-4c40-8dcc-36ee6968769f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_labels(PHOTO_MASK,PHOTO_MASK_LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ef5df-9046-4e5d-8000-e12df9765b93",
   "metadata": {},
   "source": [
    "### Label2Normalise (Camera_Frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e3720860-6089-46dc-a384-f7d99a8623cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalise_json(input_dir, scale_factors_filename):\n",
    "    # Buscar el primer archivo .txt en el directorio\n",
    "    json_file = next((f for f in os.listdir(input_dir) if f.endswith('.txt')), None)\n",
    "    if not json_file:\n",
    "        raise FileNotFoundError(\"No .txt file found in the directory.\")\n",
    "    json_path = os.path.join(input_dir, json_file)\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    with open(scale_factors_filename, 'r') as file:\n",
    "        scale_factors = json.load(file)\n",
    "    \n",
    "    for entry in data:\n",
    "        for key, value in entry.items():\n",
    "            if key == 'centroid':\n",
    "                x_key, y_key = 'centroid_x', 'centroid_y'\n",
    "                print(entry[key][0])\n",
    "                entry[key][0] = normalize_value(entry[key][0], scale_factors.get(x_key, {}))\n",
    "                print(entry[key][0])\n",
    "                entry[key][1] = normalize_value(entry[key][1], scale_factors.get(y_key, {}))\n",
    "            else:\n",
    "                if key in scale_factors and isinstance(value, (int, float)):\n",
    "                    entry[key] = normalize_value(value, scale_factors[key])\n",
    "    with open(json_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "def normalize_value(value, factor):\n",
    "    if not factor:  \n",
    "        return value\n",
    "    norm_value = (value - factor['mean']) / factor['std'] if factor['std'] else value - factor['mean']\n",
    "    return 2 * (norm_value - factor['min_after_std']) / (factor['max_after_std'] - factor['min_after_std']) - 1 if factor['max_after_std'] - factor['min_after_std'] else norm_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "039f0ef4-ce8c-4bb8-9ead-0d744dcb06d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "normalise_json(PHOTO_MASK_LABEL, PRED_SCALE_FACTORS);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bfe181-c6bf-4217-b370-9c6439e81a08",
   "metadata": {},
   "source": [
    "### CameraFrame2RobotFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c8a6c6f-dea1-4474-b269-9046f93a94ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3102, -0.0290,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000])\n"
     ]
    }
   ],
   "source": [
    "file_label = os.listdir(PHOTO_MASK_LABEL)[1]\n",
    "label_path = os.path.join(PHOTO_MASK_LABEL, file_label)\n",
    "label= [read_predicted_labels(label_path)]\n",
    "pred_labels_train = torch.tensor(label, dtype=torch.float).squeeze()\n",
    "print(pred_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b600a5ba-f36f-4abd-96c4-ce0ba1f35709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstruct_output2(scale_factors_filename, tensor_values):\n",
    "    \"\"\"\n",
    "    Correctly reconstructs the original scale of values given the path to a file containing scale factors,\n",
    "    mean, std, min, max, and a tensor of normalized values.\n",
    "    \n",
    "    Args:\n",
    "        scale_factors_filename (str): The path to the file containing the scale factors.\n",
    "        tensor_values (torch.Tensor): A tensor containing the normalized values for each dimension (x, y, z or x, y).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the values in their original scale for each dimension.\n",
    "    \"\"\"\n",
    "    # Load scale factors from the file\n",
    "    with open(scale_factors_filename, 'r') as file:\n",
    "        scale_factors = json.load(file)\n",
    "\n",
    "    # Prepare a tensor to hold the reconstructed values\n",
    "    original_values = torch.zeros_like(tensor_values)\n",
    "\n",
    "    keys = ['x', 'y', 'z'][:len(tensor_values)]\n",
    "    for i, key in enumerate(keys):\n",
    "        if key in scale_factors:\n",
    "            factor = scale_factors[key]\n",
    "            # Invert range normalization: x = (norm_val * (x_max - x_min)) + x_min\n",
    "            value_range = (tensor_values[i] * (factor['max'] - factor['min'])) + factor['min']\n",
    "            # Invert centered scaling: x = (x * std) + mean\n",
    "            original_values[i] = (value_range * factor['std']) + factor['mean']\n",
    "        else:\n",
    "            original_values[i] = tensor_values[i]\n",
    "\n",
    "    return original_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b6d30-166b-4cff-9404-9ad6f5bc2697",
   "metadata": {},
   "source": [
    "#### Weight download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "deb2a44e-5360-48e6-a1fc-dcb89705af0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4266,  0.3281, -0.0019], grad_fn=<TanhBackward0>)\n",
      "tensor([427.9802, -85.9397,   5.0000], grad_fn=<CopySlices>)\n",
      "\n",
      "\n",
      "tensor([ 0.4756,  0.5020, -0.0015], grad_fn=<TanhBackward0>)\n",
      "tensor([442.6817,   0.9943,   5.0000], grad_fn=<CopySlices>)\n",
      "\n",
      "\n",
      "tensor([ 9.9798e-01,  9.3194e-01, -4.6194e-04], grad_fn=<TanhBackward0>)\n",
      "tensor([599.3950, 215.9720,   5.0000], grad_fn=<CopySlices>)\n",
      "\n",
      "\n",
      "tensor([ 0.9919,  0.9254, -0.0024], grad_fn=<TanhBackward0>)\n",
      "tensor([597.5636, 212.7009,   5.0000], grad_fn=<CopySlices>)\n",
      "\n",
      "\n",
      "tensor([0.7252, 0.7489, 0.0290], grad_fn=<TanhBackward0>)\n",
      "tensor([517.5580, 124.4740,   5.0000], grad_fn=<CopySlices>)\n",
      "\n",
      "\n",
      "tensor([ 0.8013,  0.8202, -0.0038], grad_fn=<TanhBackward0>)\n",
      "tensor([540.3989, 160.0830,   5.0000], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "model1 = MLP(INPUT_SIZE, HIDDEN_SIZE1, OUTPUT_SIZE);\n",
    "model2 = MLP(INPUT_SIZE, HIDDEN_SIZE2, OUTPUT_SIZE);\n",
    "model3 = MLP(INPUT_SIZE, HIDDEN_SIZE1, OUTPUT_SIZE);\n",
    "model4 = MLP(INPUT_SIZE, HIDDEN_SIZE2, OUTPUT_SIZE);\n",
    "model5 = MLP(INPUT_SIZE, HIDDEN_SIZE1, OUTPUT_SIZE);\n",
    "model6 = MLP(INPUT_SIZE, HIDDEN_SIZE2, OUTPUT_SIZE);\n",
    "\n",
    "model1.load_state_dict(torch.load(MLP_1));\n",
    "model2.load_state_dict(torch.load(MLP_2));\n",
    "model3.load_state_dict(torch.load(MLP_3));\n",
    "model4.load_state_dict(torch.load(MLP_4));\n",
    "model5.load_state_dict(torch.load(MLP_5));\n",
    "model6.load_state_dict(torch.load(MLP_6));\n",
    "\n",
    "model1.eval() ;\n",
    "model2.eval() ;\n",
    "model3.eval() ;\n",
    "model4.eval() ;\n",
    "model5.eval() ;\n",
    "model6.eval() ;\n",
    "\n",
    "output1= model1(pred_labels_train)\n",
    "print(output1)\n",
    "output1=reconstruct_output2(GT_SCALE_FACTORS,output1)\n",
    "print(output1)\n",
    "print(\"\\n\")\n",
    "\n",
    "output2= model2(pred_labels_train)\n",
    "print(output2)\n",
    "output2=reconstruct_output2(GT_SCALE_FACTORS,output2)\n",
    "print(output2)\n",
    "print(\"\\n\")\n",
    "\n",
    "output3= model3(pred_labels_train)\n",
    "print(output3)\n",
    "output3=reconstruct_output2(GT_SCALE_FACTORS,output3)\n",
    "print(output3)\n",
    "print(\"\\n\")\n",
    "\n",
    "output4= model4(pred_labels_train)\n",
    "print(output4)\n",
    "output4=reconstruct_output2(GT_SCALE_FACTORS,output4)\n",
    "print(output4)\n",
    "print(\"\\n\")\n",
    "\n",
    "output5= model5(pred_labels_train)\n",
    "print(output5)\n",
    "output5=reconstruct_output2(GT_SCALE_FACTORS,output5)\n",
    "print(output5)\n",
    "print(\"\\n\")\n",
    "\n",
    "output6= model6(pred_labels_train)\n",
    "print(output6)\n",
    "output6=reconstruct_output2(GT_SCALE_FACTORS,output6)\n",
    "print(output6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e20bfa-4727-4f73-988d-cf4bd01705e0",
   "metadata": {},
   "source": [
    "#### Resuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f2381ad9-1a89-4183-a1d2-017ec77ad9be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media por dimensión: tensor([520.9296, 104.7141,   5.0000], grad_fn=<MeanBackward1>), Desviación estándar por dimensión: tensor([ 73.7324, 122.1493,   0.0000], grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "l=[output1,output2,output3,output4,output5,output6]\n",
    "concatenated_tensor = torch.stack(l)\n",
    "mean_tensor = torch.mean(concatenated_tensor, dim=0)\n",
    "std_tensor = torch.std(concatenated_tensor, dim=0)\n",
    "print(f\"Media por dimensión: {mean_tensor}, Desviación estándar por dimensión: {std_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d521e-1263-4ad3-9040-7c2e009f5748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
